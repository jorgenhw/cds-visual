{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 10 - Image search with VGG16 and K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m \u001b[39mimport\u001b[39;00m norm\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m     10\u001b[0m \u001b[39m# tensorflow\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_hub\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mhub\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "# base tools\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(\"..\"))\n",
    "\n",
    "# data analysis\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.image import (load_img, \n",
    "                                                  img_to_array)\n",
    "from tensorflow.keras.applications.vgg16 import (VGG16, \n",
    "                                                 preprocess_input)\n",
    "# from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What kind of preprocessing am I doing here? Why do you think I'm doing it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img_path, model):\n",
    "    \"\"\"\n",
    "    Extract features from image data using pretrained model (e.g. VGG16)\n",
    "    \"\"\"\n",
    "    # Define input image shape - remember we need to reshape\n",
    "    input_shape = (224, 224, 3)\n",
    "    # load image from file path\n",
    "    img = load_img(img_path, target_size=(input_shape[0], \n",
    "                                          input_shape[1]))\n",
    "    # convert to array\n",
    "    img_array = img_to_array(img)\n",
    "    # expand to fit dimensions\n",
    "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "    # preprocess image - see last week's notebook\n",
    "    preprocessed_img = preprocess_input(expanded_img_array)\n",
    "    # use the predict function to create feature representation\n",
    "    features = model.predict(preprocessed_img)\n",
    "    # flatten\n",
    "    flattened_features = features.flatten()\n",
    "    # normalise features\n",
    "    normalized_features = flattened_features / norm(features) # we are normalizing by taking the pictures and ficiding by norm(features) which is a numpy function.\n",
    "    return flattened_features # we should return normalized_features instead of flattened_features which might yield better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VGG16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we are loading our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', # use the weights from the imagenet dataset\n",
    "              include_top=False, # don't include the classfication layers. Only include the weights from the embeddings.\n",
    "              pooling='avg', # average pooling\n",
    "              input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features from single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extract_features('../data/img/florence.jpg', model) # the features are the embeddings from the VGG16 model which are a big array of numbers with shape (1, 7, 7, 512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the datasets\n",
    "root_dir = os.path.join(\"..\") # creating root directory\n",
    "filenames = sorted(get_file_list(root_dir)) # \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract features for each image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [] # i want a list of all the features extracted from the images\n",
    "for i in tqdm(range(len(filenames)), position=0, leave=True): #position and leave has to do with where the progress bar is displayed\n",
    "    feature_list.append(extract_features(filenames[i], model)) # append the features to the list (feature_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature_list contains a list of arrays of length 1360 each of which are of 512."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbours"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our *database* of extracted embeddings, we can then use K-Nearest Neighbours to find similar images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighborsw\n",
    "neighbors = NearestNeighbors(n_neighbors=10, # find the 10 nearest neighbors\n",
    "                             algorithm='brute', # use brute force algorithm\n",
    "                             metric='cosine').fit(feature_list) # once we've initialised the model, we can fit it to the data. We are using cosine similarity to measure the distance between the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculate nearest neighbours for target__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = neighbors.kneighbors([feature_list[250]]) # we are finding the 10 nearest neighbors to the 250th image in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Save indices, print data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then go through the indices and print out the a list of indices and the distances\n",
    "idxs = []\n",
    "for i in range(1,6): # look for the five closest indices\n",
    "    print(distances[0][i], indices[0][i])\n",
    "    idxs.append(indices[0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot target image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mpimg.imread(filenames[250])) # plot image 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot close images__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mpimg.imread(filenames[251]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot target and top 3 closest together__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt target\n",
    "plt.imshow(mpimg.imread(filenames[251]))\n",
    "\n",
    "# plot 3 most similar\n",
    "f, axarr = plt.subplots(1,3)\n",
    "axarr[0].imshow(mpimg.imread(filenames[idxs[0]]))\n",
    "axarr[1].imshow(mpimg.imread(filenames[idxs[1]]))\n",
    "axarr[2].imshow(mpimg.imread(filenames[idxs[2]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple style transfer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load a quick style transfer model from TF Hub__\n",
    "\n",
    "You can find more details [here](https://www.tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load TF-Hub module.\n",
    "hub_handle = 'https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2'\n",
    "hub_module = hub.load(hub_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load TF-hub module\n",
    "hub_handle = 'https://tfhub.dev/google/magenta/arbitrary-image-stylization/feature_vector/4'\n",
    "hub_module = hub.KerasLayer(hub_handle) # the handle to the module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load the content image and the style image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = st_load(\"../data/img/florence.jpg\")\n",
    "style_image = st_load(\"../data/img/starry_night.jpg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Process using the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = hub_module(content_image, style_image)\n",
    "stylized_image = outputs[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Show content, style, and stylized image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_n([content_image, style_image, stylized_image], \n",
    "       titles=['Original content image', 'Style image', 'Stylized image'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1 \n",
    "- Run this same pipeline on the Indo Fashion dataset. How does it perform?\n",
    "\n",
    "### Task 2\n",
    "- Take the code in this notebook and turn it into a Python script. You can then add this to the repo for your Assignment 1 solution for creating doing image search\n",
    "  - I.e. your Assignment 1 repo would contain both code for image search using colour histograms *and* for image search using a pretrained CNN.\n",
    "\n",
    "### Task 3 \n",
    "- Continue working on Assignment 3 in-class just now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
